# -*- coding: utf-8 -*-
"""LLaMA 3 with Flash Attention and Quantization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BInXIceRWJoazc8qpM5G_NcYHk7MZb7i
"""

!pip install huggingface_hub
!pip install accelerate

!pip install torch --upgrade

from huggingface_hub import login
login(token="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")

"""# LLaMA3 8B-instruct Model

First Load the LLaMA 3 using transformers and profile the inference time.
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.profiler import profile, record_function, ProfilerActivity

# Load model and tokenizer
model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model_original = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
).to('cuda')  # Make sure to move your model to the appropriate device

# Prepare input
messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model_original.device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]
# Define the function to profile
def generate_response():
    with record_function("model_inference"):
        outputs = model_original.generate(
            input_ids,
            max_new_tokens=256,
            eos_token_id=terminators,
            do_sample=True,
            temperature=0.6,
            top_p=0.9,
        )
    response = outputs[0][input_ids.shape[-1]:]
    print(tokenizer.decode(response, skip_special_tokens=True))

# Setup profiler
with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:
    generate_response()

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=20))

"""### Quantize the Model to 8 bits
Quantize the model, loading it in 4-bit mode. 4-bit loading takes about 7 GB of memory to run, making it compatible with a lot of consumer cards and all the GPUs in Google Colab.
"""

!pip install bitsandbytes
!pip install accelerate

!pip show bitsandbytes
!pip show accelerate

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext accelerate
# %reload_ext bitsandbytes

from transformers import pipeline
import torch
import bitsandbytes
import accelerate
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.profiler import profile, record_function, ProfilerActivity

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

pipeline = pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={
        "torch_dtype": torch.float16,
        "quantization_config": {"load_in_4bit": True},
        "low_cpu_mem_usage": True,
    },
)

messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]

prompt = pipeline.tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
)

terminators = [
    pipeline.tokenizer.eos_token_id,
    pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
]


def generate_response():
    with record_function("model_inference"):
        outputs = pipeline(
            prompt,
            max_new_tokens=256,
            eos_token_id=terminators,
            do_sample=True,
            temperature=0.6,
            top_p=0.9,
        )
    print(outputs[0]["generated_text"][len(prompt):])
# Setup profiler
with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:
    generate_response()

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))

"""## Add Flash Attention 2 for model inference

FlashAttention-2 is a faster and more efficient implementation of the standard attention mechanism that can significantly speedup inference by:

additionally parallelizing the attention computation over sequence length
partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them

Now we tried to implement the Flash attention for inference of the LLaMA 3 model.
"""

!pip install flash-attn --no-build-isolation

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)

# load in 4bit and with Flash Attention 2
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_4bit=True,
    attn_implementation="flash_attention_2",
)
# Prepare input
messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]
# Define the function to profile
def generate_response():
    with record_function("model_inference"):
        outputs = model.generate(
            input_ids,
            max_new_tokens=256,
            eos_token_id=terminators,
            do_sample=True,
            temperature=0.6,
            top_p=0.9,
        )
    response = outputs[0][input_ids.shape[-1]:]
    print(tokenizer.decode(response, skip_special_tokens=True))

# Setup profiler
with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:
    generate_response()

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=20))

messages = [
    {"role": "system", "content": "You are a helpful AI assistant who is an expert in World History. Please answer the historical questions to the best of your knowledge"},
    {"role": "user", "content": "What was the name of the series of programs and projects President Franklin D. Roosevelt enacted during The Great Depression and how does it help the US economy?"},
]
input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)
def generate_response():
    with record_function("model_inference"):
        outputs = model.generate(
            input_ids,
            max_new_tokens=256,
            eos_token_id=terminators,
            do_sample=True,
            temperature=0.6,
            top_p=0.9,
        )
    response = outputs[0][input_ids.shape[-1]:]
    print(tokenizer.decode(response, skip_special_tokens=True))

# Setup profiler
with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:
    generate_response()

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=20))

messages = [
    {"role": "system", "content": "You are a helpful AI assistant who is an expert in World History. Please answer the historical questions to the best of your knowledge"},
    {"role": "user", "content": "What was the name of the series of programs and projects President Franklin D. Roosevelt enacted during The Great Depression and how does it help the US economy?"},
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model_original.device)
# Define the function to profile
def generate_response():
    with record_function("model_inference"):
        outputs = model_original.generate(
            input_ids,
            max_new_tokens=256,
            eos_token_id=terminators,
            do_sample=True,
            temperature=0.6,
            top_p=0.9,
        )
    response = outputs[0][input_ids.shape[-1]:]
    print(tokenizer.decode(response, skip_special_tokens=True))

# Setup profiler
with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:
    generate_response()

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=20))

"""As we can see, Flash attention 2 with 4 bits quantization doesn't give me the improvement I was hoping for. Maybe the length of the input token is not long enough. At least for the inference part, the need to use Flash Attention is not necessary"""

!pip install datasets

import torch
import time
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from datasets import load_dataset
from peft import get_peft_config, get_peft_model, LoraConfig, TaskType

peft_config = LoraConfig(
    task_type=TaskType.LLM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1
)

model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
model = get_peft_model(model, peft_config)

# Load the dataset
dataset = load_dataset("tiny_shakespeare")

# Tokenization function to process the text
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

# Model and tokenizer initialization
model_name =  "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Apply tokenization to the dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)
# Load the model
model = AutoModelForCausalLM.from_pretrained(model_name).to("cuda")
# Set the model's loss function
model.loss_function = torch.nn.CrossEntropyLoss()
# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    save_strategy="epoch",
    load_best_model_at_end=True,
    logging_dir='./logs',
    report_to="tensorboard"
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test']
)

# Start timing and fine-tuning
start_time = time.time()
trainer.train()
end_time = time.time()

# Calculate and print the total training time
total_training_time = end_time - start_time
print(f"Total training time: {total_training_time:.2f} seconds")


# Save the fine-tuned model
model_path = "./fine_tuned_model"
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)

"""!pip install accelerate peft bitsandbytes transformers trl"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install accelerate peft bitsandbytes transformers trl

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
    Trainer
)
from peft import LoraConfig
from trl import SFTTrainer
import time
# Model from Hugging Face hub
base_model = "meta-llama/Meta-Llama-3-8B-Instruct"



# Fine-tuned model
new_model = "llama-2-new-no-flash-attention"
dataset = load_dataset("tiny_shakespeare", split="train")
compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=quant_config,
    device_map={"": 0}
)
model.config.use_cache = False
model.config.pretraining_tp = 1
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
peft_params = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
training_params = TrainingArguments(
    output_dir="./results",
    num_train_epochs=10,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    optim="paged_adamw_32bit",
    save_steps=25,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=False,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant",
    report_to="tensorboard"
)
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_params,
    dataset_text_field="text",
    max_seq_length=None,
    tokenizer=tokenizer,
    args=training_params,
    packing=False,
)


# Start timing and fine-tuning
start_time = time.time()
trainer.train()
end_time = time.time()

# Calculate and print the total training time
total_training_time = end_time - start_time
print(f"Total training time: {total_training_time:.2f} seconds")


# Save the fine-tuned model
model_path = "./fine_tuned_model"
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
    Trainer
)
from peft import LoraConfig
from trl import SFTTrainer
import time
# Model from Hugging Face hub
base_model = "meta-llama/Meta-Llama-3-8B-Instruct"



# Fine-tuned model
new_model = "llama-2-new-no-flash-attention"
dataset = load_dataset("tiny_shakespeare", split="train")
compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=quant_config,
    device_map={"": 0},
    attn_implementation="flash_attention_2", # add flash attention
)
model.config.use_cache = False
model.config.pretraining_tp = 1
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
peft_params = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
training_params = TrainingArguments(
    output_dir="./results",
    num_train_epochs=10,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    optim="paged_adamw_32bit",
    save_steps=25,
    logging_steps=25,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=False,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant",
    report_to="tensorboard"
)
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_params,
    dataset_text_field="text",
    max_seq_length=None,
    tokenizer=tokenizer,
    args=training_params,
    packing=False,
)


# Start timing and fine-tuning
start_time = time.time()
trainer.train()
end_time = time.time()

# Calculate and print the total training time
total_training_time = end_time - start_time
print(f"Total training time: {total_training_time:.2f} seconds")


# Save the fine-tuned model
model_path = "./fine_tuned_model"
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)